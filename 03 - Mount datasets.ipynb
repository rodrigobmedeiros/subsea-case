{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount datasets to use\n",
    "\n",
    "Notebook responsible to synchronize vessel response data and weather data into a unique dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files_into_dataframes(path, filename, date_column_name):\n",
    "    \"\"\"\n",
    "    This function read a csv file into, considering a specific column as date e set it as index.\n",
    "    \n",
    "    args:\n",
    "    path (str) -> Base directory.\n",
    "    filename (str) -> .csv file located into path.\n",
    "    date_column_name (str) -> column name with time information.\n",
    "    \n",
    "    return:\n",
    "    DataFrame -> return a dataframe with date_column_name as index and in datetime format.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    if filename.split('.')[-1] != 'csv':\n",
    "        \n",
    "        raise ValueError('Enter a valid csv filename')\n",
    "    \n",
    "    df = pd.read_csv(''.join([path, filename]), parse_dates=[date_column_name], dtype='float32')\n",
    "    df.set_index(date_column_name, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_validation_time(df, is_train=True, min_date='', max_date='', date_column_name='Date'):\n",
    "    \"\"\"\n",
    "    Filter df by validation time considering training or validation dataset.\n",
    "    \n",
    "    args:\n",
    "    \n",
    "    df (pd.DataFrame) -> data frame with complete data including all period extract from process and lab.\n",
    "    is_train (bool) -> define if df we'll be used to train or to validation.\n",
    "    min_date -> smallest date in validation period.\n",
    "    max_date -> highest date in validation period.\n",
    "    \n",
    "    returns:\n",
    "    \n",
    "    pd.DataFrame -> df filtered by date to be used to train or validade.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    if min_date == '':\n",
    "        min_date = df.index.min()\n",
    "        \n",
    "    if max_date == '':\n",
    "        max_date = df.index.max()\n",
    "        \n",
    "    is_greater_than_min_date = df.index >= min_date\n",
    "    is_smaller_than_max_date = df.index <= max_date\n",
    "    filter_date = is_greater_than_min_date & is_smaller_than_max_date\n",
    "    \n",
    "    if is_train:\n",
    "        return df[~filter_date]\n",
    "    \n",
    "    else:\n",
    "        return df[filter_date]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'data\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Interpolated weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_filename = 'weather_interpolated.csv'\n",
    "weather_data = read_files_into_dataframes(base_path, weather_filename, 'Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading resampled vessel response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessel_response_filename = 'vessel_response_agg_by_second.csv'\n",
    "vessel_response_data = read_files_into_dataframes(base_path, vessel_response_filename, 'Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount a dataset joining weather and vessel response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = pd.merge(vessel_response_data, weather_data, how='left', on='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time interval :>> 19 days 23:59:59\n",
      "From: 2020-08-15 00:00:00+00:00\n",
      "To: 2020-09-03 23:59:59+00:00\n"
     ]
    }
   ],
   "source": [
    "data_time_interval = complete_data.index.max() - complete_data.index.min()\n",
    "print(f'Total time interval :>> {data_time_interval}')\n",
    "print(f'From: {complete_data.index.min()}')\n",
    "print(f'To: {complete_data.index.max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing dataset into `train`, `test` and `validation`.\n",
    "\n",
    "The dataset will be divided into 80% for `trainning`, 10% for `test` and 10% for `validation`. \n",
    "\n",
    "- Trainning (16 days) - `15/08/2020 00:00:00` - `30/08/2020 23:59:59`\n",
    "- Test (2 days) - `31/08/2020 00:00:00` - `01/09/2020 23:59:59`\n",
    "- Validation (2 days) - `02/09/2020 00:00:00` - `03/09/2020 23:59:59`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_train = filter_validation_time(complete_data,\n",
    "                                             is_train=False,\n",
    "                                             min_date='15/08/2020 00:00:00',\n",
    "                                             max_date='30/08/2020 23:59:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_test = filter_validation_time(complete_data,\n",
    "                                             is_train=False,\n",
    "                                             min_date='08/31/2020 00:00:00',\n",
    "                                             max_date='09/01/2020 23:59:59')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_validation = filter_validation_time(complete_data,\n",
    "                                             is_train=False,\n",
    "                                             min_date='09/02/2020 00:00:00',\n",
    "                                             max_date='09/03/2020 23:59:59')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points train: 1382400\n",
      "Start date: 2020-08-15 00:00:00+00:00\n",
      "End date: 2020-08-30 23:59:59+00:00\n",
      "Percentual of data: 80.0\n",
      "Saving file...\n",
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of points train: {len(complete_data_train)}')\n",
    "print(f'Start date: {complete_data_train.index.min()}')\n",
    "print(f'End date: {complete_data_train.index.max()}')\n",
    "print(f'Percentual of data: {round((len(complete_data_train) / len(complete_data)) * 100, 2)}')\n",
    "print('Saving file...')\n",
    "train_filename = 'complete_data_train.csv'\n",
    "complete_data_train.to_csv(''.join([base_path, train_filename]))\n",
    "print('File Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points train: 172800\n",
      "Start date: 2020-08-31 00:00:00+00:00\n",
      "End date: 2020-09-01 23:59:59+00:00\n",
      "Percentual of data: 10.0\n",
      "Saving file...\n",
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of points train: {len(complete_data_test)}')\n",
    "print(f'Start date: {complete_data_test.index.min()}')\n",
    "print(f'End date: {complete_data_test.index.max()}')\n",
    "print(f'Percentual of data: {round((len(complete_data_test) / len(complete_data)) * 100, 2)}')\n",
    "print('Saving file...')\n",
    "test_filename = 'complete_data_test.csv'\n",
    "complete_data_test.to_csv(''.join([base_path, test_filename]))\n",
    "print('File Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of points train: 172800\n",
      "Start date: 2020-09-02 00:00:00+00:00\n",
      "End date: 2020-09-03 23:59:59+00:00\n",
      "Percentual of data: 10.0\n",
      "Saving file...\n",
      "File Saved\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of points train: {len(complete_data_validation)}')\n",
    "print(f'Start date: {complete_data_validation.index.min()}')\n",
    "print(f'End date: {complete_data_validation.index.max()}')\n",
    "print(f'Percentual of data: {round((len(complete_data_validation) / len(complete_data)) * 100, 2)}')\n",
    "print('Saving file...')\n",
    "valid_filename = 'complete_data_validation.csv'\n",
    "complete_data_validation.to_csv(''.join([base_path, valid_filename]))\n",
    "print('File Saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
